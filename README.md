# 30 Days of Data Cleaning with Pandas

Welcome to the **30 Days of Data Cleaning using Python & Pandas** series!  
This is a structured learning journey designed to help you master data cleaning using the Pandas library — an essential skill for any data analyst, data scientist, or Python developer working with real-world data.

---

## What You’ll Learn

- Data inspection, null handling, and type conversions  
- Detecting & fixing outliers, duplicates, and formatting issues  
- Combining, merging, grouping, and reshaping data  
- Real-world strategies for wrangling messy datasets  
- Mini projects and case studies to reinforce learning

---

## Daily Breakdown

| Day | Topic |
|-----|-------|
| Day 1  | Intro to Pandas – Why Pandas, Installation, Importing |
| Day 2  | Creating Series and DataFrames |
| Day 3  | Reading CSV, Excel, JSON, and SQL files |
| Day 4  | Data Inspection – `.head()`, `.tail()`, `.info()`, `.describe()` |
| Day 5  | Selecting Columns and Rows – `loc[]`, `iloc[]` |
| Day 6  | Data Types and Conversions |
| Day 7  | Mini Project: Exploratory Analysis on Titanic Dataset |
| Day 8  | Handling Missing Data – `.isnull()`, `.fillna()`, `.dropna()` |
| Day 9  | Removing Duplicates |
| Day 10 | String Cleaning – Case, Stripping, Replacing, Regex |
| Day 11 | DateTime Cleaning – Parsing, Formatting, Extracting |
| Day 12 | Data Transformation – Apply, Map, Lambda |
| Day 13 | Combining DataFrames – `concat()`, `merge()`, `join()` |
| Day 14 | Grouping and Aggregating – `groupby()`, `.agg()` |
| Day 15 | Data Reshaping – `pivot()`, `melt()`, `stack()`, `unstack()` |
| Day 16 | Column Cleaning – Renaming, Dropping, Reordering |
| Day 17 | Index Resetting and Setting |
| Day 18 | Filtering Data with Conditions |
| Day 19 | Handling Outliers – IQR, Z-score |
| Day 20 | Feature Engineering – Creating New Columns |
| Day 21 | Binning and Categorizing Data |
| Day 22 | Scaling and Normalization |
| Day 23 | Data Validation and Assertions |
| Day 24 | Working with Categorical Data |
| Day 25 | Encoding Categorical Variables |
| Day 26 | Saving Cleaned Data – CSV, Excel, JSON |
| Day 27 | Project Setup – Cleaning a Sales Dataset |
| Day 28 | Project Work – Cleaning Continues |
| Day 29 | Final Project Review and Summary |
| Day 30 | Wrap-Up: Best Practices + Checklist

---

## Folder Structure

```

Week_01/
├── day_01_intro_to_pandas.ipynb
├── day_02_create_series_dataframes.ipynb
├── day_03_reading_files.ipynb
├── day_04_data_inspection.ipynb
├── day_05_selecting_data.ipynb
├── day_06_data_types_conversions.ipynb
├── day_07_titanic_project.ipynb

Week_02/
├── day_08_missing_data.ipynb
├── day_09_remove_duplicates.ipynb
├── day_10_string_cleaning.ipynb
├── day_11_datetime_cleaning.ipynb
├── day_12_data_transformation.ipynb
├── day_13_combining_dataframes.ipynb
├── day_14_grouping_aggregating.ipynb

Week_03/
├── day_15_data_reshaping.ipynb
├── day_16_column_cleaning.ipynb
├── day_17_index_resetting.ipynb
├── day_18_filtering_data.ipynb
├── day_19_handling_outliers.ipynb
├── day_20_feature_engineering.ipynb
├── day_21_binning_categorizing.ipynb

Week_04/
├── day_22_scaling_normalization.ipynb
├── day_23_data_validation.ipynb
├── day_24_categorical_data.ipynb
├── day_25_encoding_categorical.ipynb
├── day_26_saving_cleaned_data.ipynb
├── day_27_sales_project_setup.ipynb
├── day_28_sales_project_work.ipynb
├── day_29_final_review.ipynb
├── day_30_wrapup_best_practices.ipynb
```

---

## Setup Instructions

Install dependencies:

```bash
pip install pandas numpy jupyterlab
```

Launch the notebook:

```bash
jupyter lab
```

---

## Who Is This For?

* Beginners in data science or analytics
* Python developers who want to work with real data
* Students preparing for data-related interviews
* Anyone struggling with cleaning real-world messy data

---

## How to Use

* Go through one notebook each day
* Try modifying the examples yourself
* Apply what you learn on your own datasets
* Track your progress and share your learnings!

---

## Let's Connect

If you find this useful, feel free to star the repo, fork it, or share it.
Let’s grow and clean data together!

---